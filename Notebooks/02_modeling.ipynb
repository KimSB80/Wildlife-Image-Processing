{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cc14a2c-3a15-4ca4-a5e5-ef0492c931af",
   "metadata": {},
   "source": [
    "## Modeling with CNN for Wildlife Image Classification\n",
    "\n",
    "The data for this project is sourced from https://www.kaggle.com/datasets/akash2907/bird-species-classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaa4810-3fbd-4e84-b6f8-bb6d9a117b0b",
   "metadata": {},
   "source": [
    "### 1. Import Packages and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8fdf750-4fb5-4ee4-8b99-16375dd6b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import imageio.v2 as imageio\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db0788ed-89fe-4059-b871-a550a8e713dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04609cd-4f0c-4b75-aecb-dd76befb4f9b",
   "metadata": {},
   "source": [
    "### 2. Build model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840c7cb2-3e94-4326-8bfc-ce2fddfbf7ad",
   "metadata": {},
   "source": [
    "Loading the training dataset and applying augmentations using ImageDataGenerator. We'll need to one-hot-encode the target variable, which is done within the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2409d88f-5715-4b23-9877-3babbab9234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path and folder names\n",
    "train_path = \"../jpeg/train\"\n",
    "test_path = \"../jpeg/test\"\n",
    "\n",
    "folder_names = os.listdir(train_path)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e12f4b1d-fe18-439f-8b27-8bea626e67ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the image and batch size\n",
    "image_size = (200, 200)\n",
    "batch_size = 32\n",
    "\n",
    "# Number of classes\n",
    "num_classes = 16\n",
    "\n",
    "# Set early stopping if accuracy stops improving\n",
    "early_stopping_monitor = EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71546e9-6f45-4517-b9ca-1263aa37361a",
   "metadata": {},
   "source": [
    "#### i. Base model -- create a model with very little augmentation or preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "844bb0ec-a883-4ee9-9fee-24220d7809dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 123 images belonging to 16 classes.\n",
      "Found 26 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create an ImageDataGenerator for data augmentation and preprocessing\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,      # Normalize pixel values to [0, 1]\n",
    "    validation_split=0.2    # Split the data into 80% for training and 20% for validation\n",
    ")\n",
    "\n",
    "# Load and preprocess images from the directory\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # Set to 'categorical' for one-hot encoded labels\n",
    "    subset='training'          # Use the training subset of your data\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # Set to 'categorical' for one-hot encoded labels\n",
    "    subset='validation'        # Use the validation subset of your data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d734e09c-f825-47e7-b1d9-0b6d0ee47d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 196, 196, 32)      2432      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 98, 98, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 307328)            0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               39338112  \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 16)                2064      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 39342608 (150.08 MB)\n",
      "Trainable params: 39342608 (150.08 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model1 = Sequential()\n",
    "\n",
    "# Add a 2D Convolutional layer -- start with a small kernel size\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), padding='valid', activation='relu', input_shape=(image_size[0], image_size[1], 3)))\n",
    "\n",
    "# Add a MaxPooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add more Conv2D and MaxPooling2D layers as needed later ...\n",
    "\n",
    "# Flatten the output from Convolutional layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add a fully connected Dense layer\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Add the final Dense layer with softmax activation (ensures predictions sum to 1)\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d347c6bd-3e54-4483-bc88-89bbe9a0f446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4/4 [==============================] - 17s 4s/step - loss: 37.0858 - accuracy: 0.0732 - val_loss: 24.7217 - val_accuracy: 0.1923\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 15.0335 - accuracy: 0.1626 - val_loss: 6.4616 - val_accuracy: 0.1538\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 3.6674 - accuracy: 0.2276 - val_loss: 2.8848 - val_accuracy: 0.1154\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.1432 - accuracy: 0.3902 - val_loss: 2.6410 - val_accuracy: 0.1538\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.4997 - accuracy: 0.6911 - val_loss: 3.1919 - val_accuracy: 0.2692\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.9406 - accuracy: 0.8293 - val_loss: 3.0069 - val_accuracy: 0.3077\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.5224 - accuracy: 0.9593 - val_loss: 3.2543 - val_accuracy: 0.3077\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.2973 - accuracy: 0.9431 - val_loss: 3.4841 - val_accuracy: 0.2692\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.1515 - accuracy: 0.9837 - val_loss: 3.3572 - val_accuracy: 0.3462\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0799 - accuracy: 1.0000 - val_loss: 4.5310 - val_accuracy: 0.2692\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0431 - accuracy: 0.9919 - val_loss: 5.8245 - val_accuracy: 0.2308\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0366 - accuracy: 1.0000 - val_loss: 5.2235 - val_accuracy: 0.2692\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0192 - accuracy: 1.0000 - val_loss: 5.3277 - val_accuracy: 0.2692\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 5.8933 - val_accuracy: 0.2692\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 6.5207 - val_accuracy: 0.2692\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 5.8379 - val_accuracy: 0.2308\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 4.6236 - val_accuracy: 0.2692\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.2894 - val_accuracy: 0.2692\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.3299 - val_accuracy: 0.3462\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.3761 - val_accuracy: 0.3077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17d4a2810>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "#    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator,\n",
    "#    validation_steps=validation_generator.samples // batch_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68414fcf-22a4-46ed-8243-e4010c13ad65",
   "metadata": {},
   "source": [
    "After 18 epochs, the model accuracy is 1.00 but the validation dataset accuracy is very low (0.3077). This is a small image dataset, and this suggests that the model is overfitting the data. We'll try out data augmentation next and see how the model does. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8afbf47-a2cb-41e9-95b7-9ebcbb02bb3e",
   "metadata": {},
   "source": [
    "#### ii. Model 2 -- create a model using augmentation and preprocessing\n",
    "Use the ImageDataGenerator that augments the data by randomly shifting, rotating, and flipping the images. We'll use the same base model on this augmented data to see how it does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68c83dd0-dbc2-49f6-8ca6-1a2f83ebaea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 123 images belonging to 16 classes.\n",
      "Found 26 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create an ImageDataGenerator for data augmentation and preprocessing\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,      # Normalize pixel values to [0, 1]\n",
    "    rotation_range=20,      # Randomly rotate images within the range of 20 degrees\n",
    "    width_shift_range=0.1,  # Randomly shift images horizontally within 10% of the image width\n",
    "    height_shift_range=0.1, # Randomly shift images vertically within 10% of the image height\n",
    "    horizontal_flip=True,   # Randomly flip images horizontally\n",
    "    validation_split=0.2    # Split the data into 80% for training and 20% for validation\n",
    ")\n",
    "\n",
    "# Load and preprocess images from the directory\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # Set to 'categorical' for one-hot encoded labels\n",
    "    subset='training'          # Use the training subset of your data\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # Set to 'categorical' for one-hot encoded labels\n",
    "    subset='validation'        # Use the validation subset of your data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85d048b8-ce79-4798-8cc1-d68c02676165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 196, 196, 32)      2432      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 98, 98, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 94, 94, 32)        25632     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 47, 47, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 43, 43, 32)        25632     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 21, 21, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 21, 21, 32)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 14112)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1806464   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                6450      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                816       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1867426 (7.12 MB)\n",
      "Trainable params: 1867426 (7.12 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Use the same sequential model, but this time with augmented data\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ede9bf41-edf2-4c37-83fe-89ab036ea7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4/4 [==============================] - 18s 5s/step - loss: 3.1579 - accuracy: 0.1545 - val_loss: 2.9714 - val_accuracy: 0.2308\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.2861 - accuracy: 0.2927 - val_loss: 2.8493 - val_accuracy: 0.3462\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.2170 - accuracy: 0.2927 - val_loss: 2.5735 - val_accuracy: 0.2692\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1699 - accuracy: 0.2764 - val_loss: 2.5558 - val_accuracy: 0.3077\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0440 - accuracy: 0.3171 - val_loss: 2.4582 - val_accuracy: 0.3462\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9351 - accuracy: 0.3984 - val_loss: 2.6110 - val_accuracy: 0.3462\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8352 - accuracy: 0.4309 - val_loss: 2.3791 - val_accuracy: 0.3462\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7886 - accuracy: 0.4228 - val_loss: 2.2303 - val_accuracy: 0.4231\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6311 - accuracy: 0.5041 - val_loss: 2.1558 - val_accuracy: 0.2692\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 294s 4s/step - loss: 1.5358 - accuracy: 0.4715 - val_loss: 2.0732 - val_accuracy: 0.3846\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.5276 - accuracy: 0.4959 - val_loss: 2.3209 - val_accuracy: 0.3077\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.4355 - accuracy: 0.5447 - val_loss: 2.0681 - val_accuracy: 0.3846\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.2916 - accuracy: 0.5854 - val_loss: 2.1114 - val_accuracy: 0.3077\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.3744 - accuracy: 0.5285 - val_loss: 2.2650 - val_accuracy: 0.3846\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.4111 - accuracy: 0.5691 - val_loss: 2.1590 - val_accuracy: 0.4231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17d7bc7d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks = [early_stopping_monitor]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e52c75f-3cfc-4a85-a430-2a88516b42bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f41ff568-5d16-4163-95d2-8df6bd3832a0",
   "metadata": {},
   "source": [
    "#### iii. Model 3 -- create a model with more layers\n",
    "After building the base model and seeing that it performs poorly on the validation data, we'll add more layers and see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a4f0dc5-430a-412a-b8db-a793ff330fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 196, 196, 32)      2432      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 98, 98, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 94, 94, 32)        25632     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 47, 47, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 43, 43, 32)        25632     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 21, 21, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 21, 21, 32)        0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 14112)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               1806464   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 50)                6450      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 16)                816       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1867426 (7.12 MB)\n",
      "Trainable params: 1867426 (7.12 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a 2D Convolutional layer -- start with a small kernel size\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), padding='valid', activation='relu', input_shape=(image_size[0], image_size[1], 3)))\n",
    "\n",
    "# Add a MaxPooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add a 2D Convolutional layer -- start with a small kernel size\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), padding='valid', activation='relu', input_shape=(image_size[0], image_size[1], 3)))\n",
    "\n",
    "# Add a MaxPooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add a 2D Convolutional layer -- start with a small kernel size\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), padding='valid', activation='relu', input_shape=(image_size[0], image_size[1], 3)))\n",
    "\n",
    "# Add a MaxPooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add a dropout layer\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Flatten the output from Convolutional layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add a fully connected Dense layer\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Add a fully connected Dense layer\n",
    "model.add(Dense(50, activation='relu'))\n",
    "\n",
    "# Add the final Dense layer with softmax activation (ensures predictions sum to 1)\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ca2ef6c-48ca-4b17-b852-da7dba0f877a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4/4 [==============================] - 18s 5s/step - loss: 2.7493 - accuracy: 0.1382 - val_loss: 2.6140 - val_accuracy: 0.1538\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.6408 - accuracy: 0.1707 - val_loss: 2.5341 - val_accuracy: 0.2692\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.5538 - accuracy: 0.1789 - val_loss: 2.4970 - val_accuracy: 0.3077\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.4132 - accuracy: 0.2033 - val_loss: 2.4964 - val_accuracy: 0.1923\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.3037 - accuracy: 0.3008 - val_loss: 2.4680 - val_accuracy: 0.2308\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.2138 - accuracy: 0.3171 - val_loss: 2.4743 - val_accuracy: 0.2692\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1587 - accuracy: 0.3171 - val_loss: 2.6220 - val_accuracy: 0.2308\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0631 - accuracy: 0.3089 - val_loss: 2.3308 - val_accuracy: 0.3077\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0787 - accuracy: 0.2927 - val_loss: 2.4334 - val_accuracy: 0.2308\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8751 - accuracy: 0.3740 - val_loss: 2.4631 - val_accuracy: 0.2692\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9776 - accuracy: 0.3333 - val_loss: 2.1201 - val_accuracy: 0.2308\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8243 - accuracy: 0.4065 - val_loss: 2.7697 - val_accuracy: 0.3462\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6527 - accuracy: 0.4146 - val_loss: 2.6904 - val_accuracy: 0.3077\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0061 - accuracy: 0.3659 - val_loss: 2.3132 - val_accuracy: 0.3846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x285471fd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks = [early_stopping_monitor]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa176d8-1f51-4699-9775-813260fa2d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy for the training and validation data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1887f2a5-7bad-454c-b6a6-524fa6756393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
