{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cc14a2c-3a15-4ca4-a5e5-ef0492c931af",
   "metadata": {},
   "source": [
    "## Modeling with CNN for Wildlife Image Classification\n",
    "\n",
    "The data for this project is sourced from https://www.kaggle.com/datasets/akash2907/bird-species-classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaa4810-3fbd-4e84-b6f8-bb6d9a117b0b",
   "metadata": {},
   "source": [
    "### 1. Import Packages and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8fdf750-4fb5-4ee4-8b99-16375dd6b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import imageio.v2 as imageio\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db0788ed-89fe-4059-b871-a550a8e713dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2409d88f-5715-4b23-9877-3babbab9234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path and folder names\n",
    "train_path = \"../jpeg/train\"\n",
    "test_path = \"../jpeg/test\"\n",
    "\n",
    "folder_names = os.listdir(train_path)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "666111fb-8edf-4b70-adcf-6523c95597ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "# load images\n",
    "\n",
    "# Create lists to store image data and labels\n",
    "image_data=[]\n",
    "image_labels = []\n",
    "\n",
    "for folder in folder_names:\n",
    "    img_path = os.path.join(train_path, folder) \n",
    "    \n",
    "    for filename in os.listdir(img_path):\n",
    "        if filename.endswith((\".jpg\",\".JPG\")):\n",
    "            img = imageio.imread(os.path.join(img_path, filename))\n",
    "            img = cv2.resize(img, (300, 300)) #resize\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY ) #convert to grayscale\n",
    "\n",
    "            image_data.append(img)\n",
    "            image_labels.append(folder)\n",
    "\n",
    "# Convert data to numpy array\n",
    "image_data = np.array(image_data)\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69145a76-63d4-44f3-b51f-ec7837cf9f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train array: (149, 300, 300)\n",
      "Shape of x_train labels: 149\n"
     ]
    }
   ],
   "source": [
    "print('Shape of x_train array:', x_train.shape)\n",
    "print('Shape of x_train labels:', len(image_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f1f1c8-0674-4729-9f6b-5f68ad16334f",
   "metadata": {},
   "source": [
    "#### Get the label mappings\n",
    "The labels dictionary matches class names against the label indices used for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c454503e-663a-44c9-895d-2f8dd4f63e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to map image names to their corresponding integer labels\n",
    "name_to_label = {name: label for label, name in enumerate(folder_names)}\n",
    "\n",
    "# Create a list to store the integer labels for each image\n",
    "integer_labels = []\n",
    "\n",
    "# Loop through the list of image names and assign the corresponding integer label\n",
    "for name in image_labels:\n",
    "    label = name_to_label.get(name)\n",
    "    if label is not None:\n",
    "        integer_labels.append(label)\n",
    "    else:\n",
    "        # Handle the case when the image name is not found in the mapping\n",
    "        # Here, we'll set it to -1 to indicate an unknown label\n",
    "        integer_labels.append(-1)\n",
    "\n",
    "# Print the list of integer labels\n",
    "#print(integer_labels)\n",
    "print(len(integer_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d151bf4a-3d6e-49f3-a8e8-297203c94d6a",
   "metadata": {},
   "source": [
    "### Build base model\n",
    "First, we need to one-hot-encode the target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ee54166-34bc-45c9-8b94-e7a59980569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f8ada13-7db2-4c00-ba91-0a13c1fbb360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(149, 16)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert labels to one-hot encoded vectors\n",
    "num_classes = 16  \n",
    "one_hot_labels = to_categorical(integer_labels, num_classes=num_classes)\n",
    "\n",
    "# Print the one-hot encoded labels\n",
    "print(one_hot_labels)\n",
    "one_hot_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04609cd-4f0c-4b75-aecb-dd76befb4f9b",
   "metadata": {},
   "source": [
    "### 2. Load Data and Apply Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840c7cb2-3e94-4326-8bfc-ce2fddfbf7ad",
   "metadata": {},
   "source": [
    "Loading the training dataset and applying augmentations using ImageDataGenerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e12f4b1d-fe18-439f-8b27-8bea626e67ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the image and batch size\n",
    "image_size = (200, 200)\n",
    "batch_size = 32\n",
    "\n",
    "# Number of classes\n",
    "num_classes = 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71546e9-6f45-4517-b9ca-1263aa37361a",
   "metadata": {},
   "source": [
    "#### i. Base model -- create a model with very little augmentation or preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "844bb0ec-a883-4ee9-9fee-24220d7809dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 123 images belonging to 16 classes.\n",
      "Found 26 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create an ImageDataGenerator for data augmentation and preprocessing\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,      # Normalize pixel values to [0, 1]\n",
    "    validation_split=0.2    # Split the data into 80% for training and 20% for validation\n",
    ")\n",
    "\n",
    "# Load and preprocess images from the directory\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # Set to 'categorical' for one-hot encoded labels\n",
    "    subset='training'          # Use the training subset of your data\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # Set to 'categorical' for one-hot encoded labels\n",
    "    subset='validation'        # Use the validation subset of your data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d734e09c-f825-47e7-b1d9-0b6d0ee47d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 196, 196, 32)      2432      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 98, 98, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 307328)            0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               39338112  \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 16)                2064      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 39342608 (150.08 MB)\n",
      "Trainable params: 39342608 (150.08 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a 2D Convolutional layer\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), padding='valid', activation='relu', input_shape=(image_size[0], image_size[1], 3)))\n",
    "\n",
    "# Add a MaxPooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add more Conv2D and MaxPooling2D layers as needed later ...\n",
    "\n",
    "# Flatten the output from Convolutional layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add a fully connected Dense layer\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Add the final Dense layer with softmax activation for multi-class classification\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d347c6bd-3e54-4483-bc88-89bbe9a0f446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3/3 [==============================] - 12s 5s/step - loss: 25.6424 - accuracy: 0.0989\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 10s 3s/step - loss: 32.1674 - accuracy: 0.0833\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 17.4685 - accuracy: 0.1250\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 7.9461 - accuracy: 0.2637\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 3.6334 - accuracy: 0.2083\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 2.1651 - accuracy: 0.3407\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 2.0049 - accuracy: 0.5934\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.8109 - accuracy: 0.6374\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.5833 - accuracy: 0.6923\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.2123 - accuracy: 0.8242\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.9839 - accuracy: 0.7473\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 479s 238s/step - loss: 0.6331 - accuracy: 0.9375\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.4536 - accuracy: 0.9451\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.2844 - accuracy: 0.9451\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.1345 - accuracy: 0.9896\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.1383 - accuracy: 0.9780\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0926 - accuracy: 0.9890\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0466 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0277 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0112 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x29f67ae90>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68414fcf-22a4-46ed-8243-e4010c13ad65",
   "metadata": {},
   "source": [
    "After 18 epochs, the model accuracy is 1.00. This is a small image dataset, and this suggests that the model is likely overfitting the data. We'll try out data augmentation next and see how the model does. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8afbf47-a2cb-41e9-95b7-9ebcbb02bb3e",
   "metadata": {},
   "source": [
    "#### ii. Model 2 -- create a model with using augmentation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "68c83dd0-dbc2-49f6-8ca6-1a2f83ebaea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 123 images belonging to 16 classes.\n",
      "Found 26 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create an ImageDataGenerator for data augmentation and preprocessing\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,      # Normalize pixel values to [0, 1]\n",
    "    rotation_range=20,      # Randomly rotate images within the range of 20 degrees\n",
    "    width_shift_range=0.1,  # Randomly shift images horizontally within 10% of the image width\n",
    "    height_shift_range=0.1, # Randomly shift images vertically within 10% of the image height\n",
    "    horizontal_flip=True,   # Randomly flip images horizontally\n",
    "    validation_split=0.2    # Split the data into 80% for training and 20% for validation\n",
    ")\n",
    "\n",
    "# Load and preprocess images from the directory\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # Set to 'categorical' for one-hot encoded labels\n",
    "    subset='training'          # Use the training subset of your data\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # Set to 'categorical' for one-hot encoded labels\n",
    "    subset='validation'        # Use the validation subset of your data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "85d048b8-ce79-4798-8cc1-d68c02676165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 196, 196, 32)      2432      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 98, 98, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 307328)            0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               39338112  \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 16)                2064      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 39342608 (150.08 MB)\n",
      "Trainable params: 39342608 (150.08 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Use the same sequential model, but this time with augmented data\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ede9bf41-edf2-4c37-83fe-89ab036ea7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3/3 [==============================] - 12s 4s/step - loss: 5.3383 - accuracy: 0.2857\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 10s 3s/step - loss: 2.7251 - accuracy: 0.4167\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 10s 3s/step - loss: 2.3464 - accuracy: 0.2917\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 2.2734 - accuracy: 0.3407\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 2.1476 - accuracy: 0.3187\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 2.0796 - accuracy: 0.2857\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.9277 - accuracy: 0.3846\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 10s 4s/step - loss: 1.8404 - accuracy: 0.4945\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.6918 - accuracy: 0.5275\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 10s 3s/step - loss: 1.6516 - accuracy: 0.5385\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 10s 4s/step - loss: 1.5640 - accuracy: 0.5495\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 10s 3s/step - loss: 1.5165 - accuracy: 0.4835\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 9s 2s/step - loss: 1.4543 - accuracy: 0.5714\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.3975 - accuracy: 0.5604\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.3693 - accuracy: 0.5055\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.3862 - accuracy: 0.5385\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.2367 - accuracy: 0.6154\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 8s 3s/step - loss: 1.2619 - accuracy: 0.5934\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.4284 - accuracy: 0.6264\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 9s 3s/step - loss: 1.2019 - accuracy: 0.6703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x15dddee90>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ff1bf-de1b-4aca-9c4f-c7f3656c2292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8c48445-a4c9-4edd-bd78-1e9ba194471c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
